{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f0187d-ea17-4b2c-ae5d-c41b2c95b040",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PEFT Finetuning with QLoRA\n",
    "The following notebook is an examplke of performing QLoRA fine-tuning on a LLM using an instruction-following dataset. This script produces the same instruction-following adapter as shown in the AMP adapters_prebuilt directory and the CML Job \"Job for fine-tuning on Instruction Dataset\"\n",
    "\n",
    "Note: This does not run fine-tuning distributed accross multiple CML Workers, that requires launching accelerate cli specifying fine-tuning python scripts. See implementation in dsitributed_peft_scripts for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232e713-2dcd-4a48-98b8-f70f4a25be16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4e7743-47a5-46fc-8693-e996a4ec5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff26fc5c-7129-4eee-9cfa-94c8f0780a63",
   "metadata": {},
   "source": [
    "### Load the base model with 4bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09cb4f8e-b49d-4338-b8bb-b48f6d33383a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/cdsw/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/cdsw/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/home/cdsw/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "/home/cdsw/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96335985-04f6-4a56-b214-e5a1215d9adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer and base model in quantized mode\n",
    "base_model = \"bigscience/bloom-1b1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Configuration to load the model in 4bit quantized mode\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e1775-0927-4409-9f49-0f20c3e19ef0",
   "metadata": {},
   "source": [
    "### Get Peft Model with LoRA training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc936d0-dc62-4093-bbe6-4da52fe6b18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "          r=16,\n",
    "          lora_alpha=32,\n",
    "          target_modules=[\"query_key_value\", \"xxx\"],\n",
    "          lora_dropout=0.05,\n",
    "          bias=\"none\",\n",
    "          task_type=\"CAUSAL_LM\"\n",
    "      )\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea22648-5894-410e-b53b-954b040db72b",
   "metadata": {},
   "source": [
    "### Get and modify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a50cd6-b796-4c42-bfe3-6b3f5396c76a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/cdsw/.cache/huggingface/datasets/teknium___json/teknium--GPTeacher-General-Instruct-3d3eb51407944fd2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Loading cached processed dataset at /home/cdsw/.cache/huggingface/datasets/teknium___json/teknium--GPTeacher-General-Instruct-3d3eb51407944fd2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-d68d4531b7a1dc54.arrow\n"
     ]
    }
   ],
   "source": [
    "# Use only 30% of the dataset\n",
    "dataset_fraction = 30\n",
    "data = datasets.load_dataset('teknium/GPTeacher-General-Instruct', split=f'train[:{dataset_fraction}%]')\n",
    "\n",
    "# Merge function to combine two columns of the dataset to have examples that look like\n",
    "#<Instruction>: %s\n",
    "#<Input>: %s\n",
    "#<Response>: %s\n",
    "#    or\n",
    "#<Instruction>: %s\n",
    "#<Response>: %s\n",
    "def merge_columns(example):\n",
    "    if example[\"input\"]:\n",
    "      prediction_format = \"\"\"<Instruction>: %s\n",
    "<Input>: %s\n",
    "<Response>: %s\"\"\"\n",
    "      example[\"prediction\"] = prediction_format %(example[\"instruction\"], example[\"input\"], example[\"response\"])\n",
    "    else:\n",
    "      prediction_format = \"\"\"<Instruction>: %s\n",
    "<Response>: %s\"\"\"\n",
    "      example[\"prediction\"] = prediction_format %(example[\"instruction\"], example[\"response\"])\n",
    "    return example\n",
    "\n",
    "finetuning_data = data.map(merge_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2accba8-67c7-48e7-aa4b-aa94217ab574",
   "metadata": {},
   "source": [
    "### Set up SFTTrainer for PEFT fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0bbf88b-f90d-4c27-a17c-5062c893d447",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "                output_dir=\"outputs\",\n",
    "                num_train_epochs=1,\n",
    "                optim=\"paged_adamw_32bit\",\n",
    "                per_device_train_batch_size=1, \n",
    "                gradient_accumulation_steps=4,\n",
    "                warmup_ratio=0.03, \n",
    "                max_grad_norm=0.3,\n",
    "                learning_rate=2e-4, \n",
    "                fp16=True,\n",
    "                logging_steps=1,\n",
    "                lr_scheduler_type=\"constant\",\n",
    "                disable_tqdm=True,\n",
    "                report_to='tensorboard',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model, \n",
    "    train_dataset=finetuning_data,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field = \"prediction\",\n",
    "    packing=True,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c83c3f-7436-4333-9abb-cf093f1294e6",
   "metadata": {},
   "source": [
    "### Launch fine-tuning\n",
    "Fine-tuning takes approximately 14 minutes on a V100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8304b371-02e4-4c76-87f2-a45eb6dd24ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5416, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.4226, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.5232, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2795, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.3107, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.3264, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2317, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.1416, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2079, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.3161, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2311, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.097, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.0517, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.0757, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.0103, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.1402, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2105, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.1347, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2096, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.148, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2247, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.037, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.1169, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2086, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.1094, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.9999, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.2464, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.0162, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.9229, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.1107, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.0847, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.1569, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.976, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.015, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0327, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9381, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9297, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0608, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0063, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9708, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0405, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0578, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9248, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9451, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9159, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0796, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9962, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.2047, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9818, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1127, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9804, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0135, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9313, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0143, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9471, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.091, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8729, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9403, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9715, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9116, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9804, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8077, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9328, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0403, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9206, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8875, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9958, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9103, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0862, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0138, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8566, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1312, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0859, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9742, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0183, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9271, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8826, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.1037, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0259, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9465, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.012, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9868, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0176, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9635, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9939, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0468, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9992, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0363, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0568, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0551, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0191, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.7977, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0138, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9383, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0452, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9237, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9803, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.871, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.9865, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 1.8705, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 2.0526, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0563, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0039, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9228, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8512, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9685, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9874, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9729, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9706, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9376, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9537, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8866, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.1314, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8867, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8922, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.997, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0212, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.031, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0782, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9604, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8872, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.939, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9831, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.991, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0162, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8526, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8829, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0264, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0112, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9206, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9558, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.1003, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.971, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9449, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9692, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.1496, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8979, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8705, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9116, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9712, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9644, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9999, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8607, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9198, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9336, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0123, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8301, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9341, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0031, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.991, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8455, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9898, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.7652, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8932, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8694, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0805, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.882, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8917, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.0327, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.1374, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9563, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9341, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.7218, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.89, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.8375, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 2.1033, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.9353, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 1.965, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9523, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9919, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.909, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.053, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8708, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0233, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0893, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9706, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0224, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9385, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9734, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8934, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.901, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8934, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9203, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.1126, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8839, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9633, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9401, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.7822, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8635, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.825, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.824, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9429, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.928, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0418, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9243, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9866, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8853, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9363, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9827, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0639, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9051, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0994, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9219, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9587, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0191, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9858, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0661, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9824, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0813, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9892, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9399, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9441, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9546, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0304, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0988, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8632, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.1077, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9866, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9866, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9444, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0019, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9105, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9876, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.1053, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0218, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.871, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9255, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0228, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 2.0343, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8903, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9549, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9067, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9687, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.983, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.9334, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.038, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0466, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0439, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9769, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8917, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0137, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0018, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.03, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0889, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9398, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8979, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9245, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0427, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9534, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0779, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9551, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0331, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9196, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0775, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0992, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9536, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.877, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9462, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.1094, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0018, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.1391, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9702, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9483, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9179, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0302, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.068, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9825, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.808, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.19, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0459, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.1399, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9308, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9683, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9445, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9279, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9253, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9983, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0753, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9302, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0304, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8692, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.003, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9212, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8733, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.042, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9692, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9451, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9461, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0216, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9741, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0509, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9234, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0843, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8969, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9853, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0501, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9814, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8488, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.9426, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0878, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8954, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 2.0429, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.945, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9895, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0434, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9175, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9669, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0519, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.881, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0231, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9636, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9213, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9795, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8663, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9289, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9434, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0185, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9575, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0046, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9948, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9376, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.905, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8911, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9985, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9707, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9258, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0967, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0231, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9102, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.1048, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8682, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8975, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.95, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9689, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0564, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9842, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9371, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0008, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.216, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.95, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0166, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9177, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9455, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9988, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9057, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.933, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8549, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0279, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9395, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9312, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9346, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9047, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9259, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9456, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0148, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9238, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.951, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8787, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0396, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9878, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0279, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9316, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.7891, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8926, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0234, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0894, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9987, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.9571, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 2.0197, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.919, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9189, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9894, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8728, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9415, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9463, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9569, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.7855, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9335, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9602, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9124, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8735, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9016, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0683, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9862, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0357, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0864, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0439, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8621, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9291, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0619, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9832, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9229, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9901, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9205, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9431, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8641, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0206, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9337, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9186, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8426, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0135, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8769, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0317, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9567, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9056, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9371, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0109, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9098, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.019, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0585, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8643, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0637, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9602, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9021, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8266, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9789, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8781, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9003, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8404, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9993, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9778, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9399, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0514, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8901, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9466, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8562, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8898, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9331, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0345, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9697, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.8766, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0286, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0975, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9299, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 1.9275, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 2.0873, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.951, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9995, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.938, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8877, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9302, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.091, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9988, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9915, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0023, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.051, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9965, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0809, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.87, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9842, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0934, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.984, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0248, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0266, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9502, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0293, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9714, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8813, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9264, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9381, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9646, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8664, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8602, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9687, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0078, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9643, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8447, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0254, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0018, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9468, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9648, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9313, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9944, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9396, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.057, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9696, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9506, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0087, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.004, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.1354, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0081, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8571, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9473, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9618, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0125, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.912, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.1169, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9186, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9414, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9966, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8366, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9867, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9006, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0116, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9924, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9382, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9061, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9455, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8712, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.1028, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.9519, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 2.0656, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.8995, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0333, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0194, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0116, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.863, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0521, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0251, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0388, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8581, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0453, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9637, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9507, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.1202, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9961, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9456, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8748, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.971, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8414, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9664, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9954, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8744, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.893, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9219, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9365, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8851, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9331, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.908, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9012, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8795, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0878, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.017, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.886, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9979, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0006, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0313, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8241, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8873, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0168, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9589, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.949, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0526, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9149, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9631, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9372, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0352, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9086, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9452, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0056, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9766, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.057, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.7733, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8925, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0346, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8192, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9845, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0561, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9798, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0041, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9065, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9073, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0329, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.7276, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 2.0139, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9586, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.922, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8617, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9626, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.9918, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9411, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9923, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9716, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7305, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7737, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.914, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8883, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8679, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.866, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9796, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8958, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8542, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8406, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7893, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.877, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8015, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8828, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8826, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9412, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8508, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8016, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8551, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8452, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9436, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7077, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9477, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8676, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7506, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7854, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7912, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7725, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8232, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.834, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7753, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8243, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.906, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7777, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9438, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7898, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8097, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9008, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9051, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8089, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.6758, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7921, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9448, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7697, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9129, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7462, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7861, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8075, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8667, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8644, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8165, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.9131, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8255, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8348, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7307, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8835, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8908, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8408, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7866, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.8794, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7648, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.887, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 1.7093, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8501, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8774, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7769, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7238, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8488, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9926, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8855, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8277, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9336, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8163, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8922, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8502, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.789, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8814, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8137, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7946, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 2.0073, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.901, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9067, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8751, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9089, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.905, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7661, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7445, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9213, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8828, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9869, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7754, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9209, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8945, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9349, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9277, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8989, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8168, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7733, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 2.006, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.743, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7806, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7965, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8099, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8571, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8171, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7967, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 2.0363, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9046, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7821, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8167, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8711, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9764, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9834, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8421, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8035, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7711, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7553, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8288, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.831, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7782, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9181, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9575, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8397, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8552, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.9574, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8467, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.7189, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8438, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.8913, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 1.747, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8117, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8383, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9806, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8248, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9469, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.776, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.887, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 2.0016, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7793, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9002, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.6809, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9525, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.883, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8675, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.905, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9794, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7482, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8621, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.694, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7922, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8528, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8569, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9348, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8361, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9407, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7118, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8738, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8159, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9198, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8067, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7284, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7912, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9065, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8652, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 2.0665, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8026, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7977, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7926, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.873, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8544, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.77, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7884, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8169, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.6893, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8829, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8177, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9436, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8195, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8348, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8144, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.6354, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7415, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8266, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7659, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7944, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8754, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7893, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9702, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8134, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9985, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9291, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.824, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8998, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8799, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.9268, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.8241, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 2.0003, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.883, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9107, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.7757, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9173, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8915, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.855, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8086, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8211, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8739, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8371, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9524, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8892, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8414, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8501, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9095, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8548, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8617, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9826, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9181, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8086, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8718, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8359, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8454, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.822, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8252, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8235, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9993, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9261, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9604, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9257, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8758, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9369, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.7722, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9385, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.895, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.7624, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8226, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.7851, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9558, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 2.0336, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.7701, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9032, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.996, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.846, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8779, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8419, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9449, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8319, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 2.0086, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8643, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8537, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9072, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8853, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8403, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9063, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.961, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9504, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8168, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8634, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8995, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.9794, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.7342, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8304, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8622, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 1.8927, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 2.0065, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.7658, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9219, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9307, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.8666, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.7972, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.8519, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.8083, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9643, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.9186, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.8856, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 1.78, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'train_runtime': 845.3739, 'train_samples_per_second': 31.676, 'train_steps_per_second': 7.918, 'train_loss': 1.9399923396841534, 'epoch': 0.13}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=848, training_loss=1.9399923396841534, metrics={'train_runtime': 845.3739, 'train_samples_per_second': 31.676, 'train_steps_per_second': 7.918, 'train_loss': 1.9399923396841534, 'epoch': 0.13})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384159a-a6f7-41f5-a2d5-23f10354a494",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save adapter\n",
    "NOTE: sfttrainer savemodel() saves the adapter only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84bb5ddc-0017-4b9d-bb6a-a31a71a76b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"adapters_custom/bloom1b1-lora-instruct-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76a013-642e-4a57-a364-4cefb01651a1",
   "metadata": {},
   "source": [
    "### Reset CUDA device for inferencing\n",
    "Removing the original loaded quantized model to free up room on GPU and load the model normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86151a0b-5d83-4ebd-9eb6-6d25b746fa26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del trainer\n",
    "del model\n",
    "del tokenizer\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd02717-07f9-43fc-83a7-78b3794ced51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a96002-f40b-48c1-ac0c-88db6b86988e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b1\", return_dict=True, device_map='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb8868-be86-4d55-b82b-1e51e60caa9a",
   "metadata": {},
   "source": [
    "### Load adapter for use with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23415afd-f153-4ab6-b398-859c97d67398",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model=model, model_id=\"adapters_custom/bloom1b1-lora-instruct-notebook\", adapter_name=\"bloom1b1-lora-instruct-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0ec4c-9891-41a0-91de-eb3ed9fb91da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39fa10de-ee6e-4690-8371-758fdb4cbe89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"<Instruction>: Classify the following items into two categories: fruits and vegetables.\n",
    "<Input>: tomato, apple, cucumber, carrot, banana, zucchini, strawberry, cauliflower\n",
    "<Response>:\"\"\"\n",
    "batch = tokenizer(prompt, return_tensors='pt')\n",
    "batch = batch.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6851e5d-c020-4b5f-9e26-5a3578c32d70",
   "metadata": {},
   "source": [
    "#### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5af0669-a033-41b1-8fc6-59938246aaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " green, yellow, red, orange, red, yellow, green, blue, yellow, red, orange, red, yellow, green, blue, yellow, red, orange, red, yellow, green, blue, yellow, red, orange, red, yellow, green, blue, yellow,\n"
     ]
    }
   ],
   "source": [
    "# Inference with base model only:\n",
    "\n",
    "with model.disable_adapter():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = model.generate(**batch, max_new_tokens=60)\n",
    "    prompt_length = len(prompt)\n",
    "    print(tokenizer.decode(output_tokens[0], skip_special_tokens=True)[prompt_length:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617d949-076e-476e-9300-13a1246f75df",
   "metadata": {},
   "source": [
    "#### Fine-tuned adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed4901b-1fb8-4660-84c7-a80bf5e29e95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fruits: Tomato, Apple, Cucumber, Carrot, Banana, Zucchini, Strawberry, Cauliflower. Vegetables: Tomato, Apple, Cucumber, Carrot, Banana, Zucchini, Strawberry, Cauliflower\n"
     ]
    }
   ],
   "source": [
    "# Inference with fine-tuned adapter:\n",
    "model.set_adapter(\"bloom1b1-lora-instruct-notebook\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=60)\n",
    "prompt_length = len(prompt)\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True)[prompt_length:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f089f23-db03-478d-a601-bdca9f116c59",
   "metadata": {},
   "source": [
    "#### The finetuned adapter output is not perfect, but it is a step closer in the direction of downstream task completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
